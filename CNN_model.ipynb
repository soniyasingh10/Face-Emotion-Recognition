{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " CNN model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Suraj110597/Face_Emotion_Detection/blob/main/CNN_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzX6ZsK1L_Vn"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OwsHO8QtVyz",
        "outputId": "041e0ec4-c4d6-4d60-8ff1-c0b1b49c71d5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BJyp0MGkp3e"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Flatten, Dense, Dropout\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.optimizers import SGD\n",
        "import cv2\n",
        "import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h81weyeHi9Y5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Flatten, Dense, Dropout\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.optimizers import SGD\n",
        "import cv2\n",
        "from keras.preprocessing.image import load_img, img_to_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I46oqk2suvRl"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjKkxz5oYrGN"
      },
      "source": [
        "pip install facial_emotion_recognition"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMP0VMZ8c4q1"
      },
      "source": [
        "vid=cv2.VideoCapture(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpLVWwqqc6EC"
      },
      "source": [
        "face_cascade=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8zu7Y6hxisi"
      },
      "source": [
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense,Input,Dropout,GlobalAveragePooling2D,Flatten,Conv2D,BatchNormalization,Activation,MaxPooling2D\n",
        "from keras.models import Model,Sequential\n",
        "from keras.optimizers import Adam,SGD,RMSprop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SzIj4m0xnYR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8818d4a6-b1f0-439d-db23-20feba7fbd0e"
      },
      "source": [
        "\n",
        "picture_size = 48\n",
        "\n",
        "batch_size  = 128\n",
        "\n",
        "datagen_train  = ImageDataGenerator()\n",
        "datagen_val = ImageDataGenerator()\n",
        "\n",
        "train_set = datagen_train.flow_from_directory(\"/content/drive/MyDrive/Face Emotion Detection/images/train \",\n",
        "                                              target_size = (picture_size,picture_size),\n",
        "                                              color_mode = \"grayscale\",\n",
        "                                              batch_size=batch_size,\n",
        "                                              class_mode='categorical',\n",
        "                                              shuffle=True)\n",
        "\n",
        "\n",
        "test_set = datagen_val.flow_from_directory(\"/content/drive/MyDrive/Face Emotion Detection/images/validation\",\n",
        "                                              target_size = (picture_size,picture_size),\n",
        "                                              color_mode = \"grayscale\",\n",
        "                                              batch_size=batch_size,\n",
        "                                              class_mode='categorical',\n",
        "                                              shuffle=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 28709 images belonging to 7 classes.\n",
            "Found 7066 images belonging to 7 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zSsXN7I8kZC",
        "outputId": "8d165585-a928-491c-e833-a24a5503096f"
      },
      "source": [
        "\n",
        "from keras.optimizers import Adam,SGD,RMSprop\n",
        "\n",
        "\n",
        "no_of_classes = 7\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "#1st CNN layer\n",
        "model.add(Conv2D(64,(3,3),padding = 'same',input_shape = (48,48,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "#2nd CNN layer\n",
        "model.add(Conv2D(128,(5,5),padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "model.add(Dropout (0.25))\n",
        "\n",
        "#3rd CNN layer\n",
        "model.add(Conv2D(512,(3,3),padding = 'same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "model.add(Dropout (0.25))\n",
        "\n",
        "#4th CNN layer\n",
        "model.add(Conv2D(512,(3,3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "#Fully connected 1st layer\n",
        "model.add(Dense(256))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "# Fully connected layer 2nd layer\n",
        "model.add(Dense(512))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(no_of_classes, activation='softmax'))\n",
        "\n",
        "\n",
        "\n",
        "opt = Adam(lr = 0.0001)\n",
        "model.compile(optimizer=opt,loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 48, 48, 64)        640       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 48, 48, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 48, 48, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 24, 24, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 24, 24, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 12, 12, 512)       590336    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 12, 12, 512)       2048      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 12, 12, 512)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 6, 6, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 6, 6, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 6, 6, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 6, 6, 512)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               1179904   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 7)                 3591      \n",
            "=================================================================\n",
            "Total params: 4,478,727\n",
            "Trainable params: 4,474,759\n",
            "Non-trainable params: 3,968\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4BDTIok86wW"
      },
      "source": [
        "from keras.optimizers import RMSprop,SGD,Adam\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"/content/drive/MyDrive/Face Emotion Detection/model.h5\", monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                          min_delta=0,\n",
        "                          patience=3,\n",
        "                          verbose=1,\n",
        "                          restore_best_weights=True\n",
        "                          )\n",
        "\n",
        "reduce_learningrate = ReduceLROnPlateau(monitor='val_loss',\n",
        "                              factor=0.2,\n",
        "                              patience=3,\n",
        "                              verbose=1,\n",
        "                              min_delta=0.0001)\n",
        "\n",
        "callbacks_list = [early_stopping,checkpoint,reduce_learningrate]\n",
        "\n",
        "epochs = 48\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer = Adam(lr=0.001),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQXKTbY78mQ9",
        "outputId": "cb5aba72-fc2b-4540-fa89-142d7eef637e"
      },
      "source": [
        "history = model.fit(train_set,steps_per_epoch=train_set.n//train_set.batch_size,epochs=epochs,\n",
        "                                validation_data = test_set,\n",
        "                                validation_steps = test_set.n//test_set.batch_size,\n",
        "                                callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/48\n",
            "224/224 [==============================] - 11222s 50s/step - loss: 1.9295 - accuracy: 0.2657 - val_loss: 1.6913 - val_accuracy: 0.3531\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 2/48\n",
            "224/224 [==============================] - 1494s 7s/step - loss: 1.4697 - accuracy: 0.4303 - val_loss: 1.5026 - val_accuracy: 0.4321\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 3/48\n",
            "224/224 [==============================] - 1496s 7s/step - loss: 1.2899 - accuracy: 0.5043 - val_loss: 1.4770 - val_accuracy: 0.4511\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 4/48\n",
            "224/224 [==============================] - 1497s 7s/step - loss: 1.1866 - accuracy: 0.5413 - val_loss: 1.0682 - val_accuracy: 0.6055\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 5/48\n",
            "224/224 [==============================] - 1438s 6s/step - loss: 1.1228 - accuracy: 0.5723 - val_loss: 1.0502 - val_accuracy: 0.6003\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 6/48\n",
            "224/224 [==============================] - 1487s 7s/step - loss: 1.0641 - accuracy: 0.5914 - val_loss: 1.0750 - val_accuracy: 0.5805\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 7/48\n",
            "224/224 [==============================] - 1538s 7s/step - loss: 1.0092 - accuracy: 0.6186 - val_loss: 1.0388 - val_accuracy: 0.5957\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 8/48\n",
            "224/224 [==============================] - 1505s 7s/step - loss: 0.9672 - accuracy: 0.6336 - val_loss: 0.9196 - val_accuracy: 0.6506\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 9/48\n",
            "224/224 [==============================] - 1516s 7s/step - loss: 0.9059 - accuracy: 0.6600 - val_loss: 0.8643 - val_accuracy: 0.6798\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 10/48\n",
            "224/224 [==============================] - 1498s 7s/step - loss: 0.8850 - accuracy: 0.6677 - val_loss: 0.9475 - val_accuracy: 0.6447\n",
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
            "Epoch 11/48\n",
            "  7/224 [..............................] - ETA: 22:26 - loss: 0.8651 - accuracy: 0.6680"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAMLH8FZQFHf"
      },
      "source": [
        "from google.colab import files\n",
        "import pickle as pi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tidl2w_Bddht",
        "outputId": "16c2d6bd-011e-4c98-e03f-4ed3907ea446"
      },
      "source": [
        "pip install opencv-python==3.4.2.17"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python==3.4.2.17 in /usr/local/lib/python3.7/dist-packages (3.4.2.17)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python==3.4.2.17) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iR3IRM-GdIZv",
        "outputId": "cb82c24b-5612-4dde-c83b-72f9a3b2ccd0"
      },
      "source": [
        "import cv2\n",
        "print(cv2.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkx2DZjVhCST"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "from keras.models import load_model\n",
        "from time import sleep\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing import image\n",
        "import cv2\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVd3g31CiXGK"
      },
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrSLOjUSwEsW"
      },
      "source": [
        "face_classifier = cv2.CascadeClassifier('/content/drive/MyDrive/Face Emotion Detection/haarcascade_frontalface_default.xml')\n",
        "classifier =load_model(r'/content/drive/MyDrive/Face Emotion Detection/model.h5')\n",
        "\n",
        "emotion_labels = ['Angry','Disgust','Fear','Happy','Neutral', 'Sad', 'Surprise']\n",
        "\n",
        "cap = cv2.VideoCapture(\"/content/drive/MyDrive/Face Emotion Detection/WIN_20210501_15_47_51_Pro.mp4\")\n",
        "\n",
        "\n",
        "\n",
        "while True:\n",
        "    _, frame = cap.read()\n",
        "    labels = []\n",
        "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_classifier.detectMultiScale(gray)\n",
        "\n",
        "    for (x,y,w,h) in faces:\n",
        "        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,255),2)\n",
        "        roi_gray = gray[y:y+h,x:x+w]\n",
        "        roi_gray = cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)\n",
        "\n",
        "\n",
        "\n",
        "        if np.sum([roi_gray])!=0:\n",
        "            roi = roi_gray.astype('float')/255.0\n",
        "            roi = img_to_array(roi)\n",
        "            roi = np.expand_dims(roi,axis=0)\n",
        "\n",
        "            prediction = classifier.predict(roi)[0]\n",
        "            label=emotion_labels[prediction.argmax()]\n",
        "            label_position = (x,y)\n",
        "            cv2.putText(frame,label,label_position,cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
        "        else:\n",
        "            cv2.putText(frame,'No Faces',(30,80),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
        "    cv2_imshow(frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}